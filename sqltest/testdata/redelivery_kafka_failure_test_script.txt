-- This script tests the case where Kafka has failed, then becomes available again but attempts to deliver messages;
-- as offsets hadn't been committed in Kafka. In this case we should reject duplicates as we also record;
-- committed offsets;

--create topic testtopic;
create source test_source_1(
    col0 bigint,
    col1 tinyint,
    col2 int,
    col3 double,
    col4 decimal(10, 2),
    col5 varchar,
    col6 timestamp(6),
    primary key (col0)
) with (
    brokername = "testbroker",
    topicname = "testtopic",
    headerencoding = "json",
    keyencoding = "json",
    valueencoding = "json",
    columnselectors = (
        "k.k0",
        "v.v1",
        "v.v2",
        "v.v3",
        "v.v4",
        "v.v5",
        "v.v6"
    )
    properties = (
        "prop1" = "val1",
        "prop2" = "val2"
    )
);

--load data dataset_1;
select * from test_source_1 order by col0;

-- simulate failure of Kafka - this will cause consumers to be closed and restarted;
--kafka fail testtopic test_source_1 1000;

-- reset the offsets and load the data;
--reset offsets testtopic;
--load data dataset_2;

-- only the last 5 records from dataset 2 should be present as the first 10 have the same PK as in dataset one;
select * from test_source_1 order by col0;
drop source test_source_1;
--delete topic testtopic;