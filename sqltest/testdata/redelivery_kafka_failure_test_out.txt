--create topic testtopic;
use test;
0 rows returned
create source test_source_1(
    col0 bigint,
    col1 tinyint,
    col2 int,
    col3 double,
    col4 decimal(10, 2),
    col5 varchar,
    col6 timestamp(6),
    primary key (col0)
) with (
    brokername = "testbroker",
    topicname = "testtopic",
    headerencoding = "json",
    keyencoding = "json",
    valueencoding = "json",
    columnselectors = (
        meta("key").k0,
        v1,
        v2,
        v3,
        v4,
        v5,
        v6
    ),
    properties = (
        "prop1" = "val1",
        "prop2" = "val2"
    )
);
0 rows returned

-- this will disable committing of offsets in Kafka, but they will be committed in Prana;
--disable commit offsets test_source_1;

--load data dataset_1;

select * from test_source_1 order by col0;
+---------------------------------------------------------------------------------------------------------------------+
| col0                 | col1 | col2        | col3         | col4         | col5         | col6                       |
+---------------------------------------------------------------------------------------------------------------------+
| 1                    | 10   | 1000        | 1234.432100  | 12345678.99  | str1         | 2020-01-01 01:00:00.123456 |
| 2                    | 20   | 2000        | 2234.432100  | 22345678.99  | str2         | 2020-01-02 01:00:00.123456 |
| 3                    | 30   | 3000        | 3234.432100  | 32345678.99  | str3         | 2020-01-03 01:00:00.123456 |
| 4                    | 40   | 4000        | 4234.432100  | 42345678.99  | str4         | 2020-01-04 01:00:00.123456 |
| 5                    | 50   | 5000        | 5234.432100  | 52345678.99  | str5         | 2020-01-05 01:00:00.123456 |
| 6                    | 60   | 6000        | 6234.432100  | 62345678.99  | str6         | 2020-01-06 01:00:00.123456 |
| 7                    | 70   | 7000        | 7234.432100  | 72345678.99  | str7         | 2020-01-07 01:00:00.123456 |
| 8                    | 80   | 8000        | 8234.432100  | 82345678.99  | str8         | 2020-01-08 01:00:00.123456 |
| 9                    | 90   | 9000        | 9234.432100  | 92345678.99  | str9         | 2020-01-09 01:00:00.123456 |
| 10                   | 100  | 10000       | 10234.432100 | 93345678.99  | str10        | 2020-01-10 01:00:00.123456 |
+---------------------------------------------------------------------------------------------------------------------+
10 rows returned

-- simulate failure of Kafka - this will cause consumers to be closed and restarted until they reconnect after 1 second;
--kafka fail testtopic test_source_1 1000;

-- re-enable committing of offsets;
--enable commit offsets test_source_1;

-- when the consumers reconnect they will consume again from the last committed offset, since we disabled committing offsets earlier;
-- so they all the messages will be received again but they will be rejected as duplicates;
-- there should be at least 10 duplicates - there could be more in the case of a rebalance occurring and messages consumed
-- between reconnecting and rebalance occurring;
--wait for committed test_source_1 20;

-- send some more data;
--load data dataset_2;
select * from test_source_1 order by col0;
+---------------------------------------------------------------------------------------------------------------------+
| col0                 | col1 | col2        | col3         | col4         | col5         | col6                       |
+---------------------------------------------------------------------------------------------------------------------+
| 1                    | 10   | 1000        | 1234.432100  | 12345678.99  | str1         | 2020-01-01 01:00:00.123456 |
| 2                    | 20   | 2000        | 2234.432100  | 22345678.99  | str2         | 2020-01-02 01:00:00.123456 |
| 3                    | 30   | 3000        | 3234.432100  | 32345678.99  | str3         | 2020-01-03 01:00:00.123456 |
| 4                    | 40   | 4000        | 4234.432100  | 42345678.99  | str4         | 2020-01-04 01:00:00.123456 |
| 5                    | 50   | 5000        | 5234.432100  | 52345678.99  | str5         | 2020-01-05 01:00:00.123456 |
| 6                    | 60   | 6000        | 6234.432100  | 62345678.99  | str6         | 2020-01-06 01:00:00.123456 |
| 7                    | 70   | 7000        | 7234.432100  | 72345678.99  | str7         | 2020-01-07 01:00:00.123456 |
| 8                    | 80   | 8000        | 8234.432100  | 82345678.99  | str8         | 2020-01-08 01:00:00.123456 |
| 9                    | 90   | 9000        | 9234.432100  | 92345678.99  | str9         | 2020-01-09 01:00:00.123456 |
| 10                   | 100  | 10000       | 10234.432100 | 93345678.99  | str10        | 2020-01-10 01:00:00.123456 |
| 11                   | 121  | 11000       | 11234.432100 | 62345678.99  | str6         | 2020-01-06 01:00:00.123456 |
| 12                   | 122  | 12000       | 12234.432100 | 72345678.99  | str7         | 2020-01-07 01:00:00.123456 |
| 13                   | 123  | 13000       | 13234.432100 | 82345678.99  | str8         | 2020-01-08 01:00:00.123456 |
| 14                   | 124  | 14000       | 14234.432100 | 92345678.99  | str9         | 2020-01-09 01:00:00.123456 |
| 15                   | 125  | 15000       | 15234.432100 | 93345678.99  | str10        | 2020-01-10 01:00:00.123456 |
+---------------------------------------------------------------------------------------------------------------------+
15 rows returned

drop source test_source_1;
0 rows returned
--delete topic testtopic;
