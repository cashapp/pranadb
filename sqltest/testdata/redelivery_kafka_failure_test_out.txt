-- This script tests the case where Kafka has failed, then becomes available again but attempts to deliver messages;
-- as offsets hadn't been committed in Kafka. In this case we should reject duplicates as we also record;
-- committed offsets;

--create topic testtopic;
use test;
0 rows returned
create source test_source_1(
    col0 bigint,
    col1 tinyint,
    col2 int,
    col3 double,
    col4 decimal(10, 2),
    col5 varchar,
    col6 timestamp(6),
    primary key (col0)
) with (
    brokername = "testbroker",
    topicname = "testtopic",
    headerencoding = "json",
    keyencoding = "json",
    valueencoding = "json",
    columnselectors = (
        "k.k0",
        "v.v1",
        "v.v2",
        "v.v3",
        "v.v4",
        "v.v5",
        "v.v6"
    )
    properties = (
        "prop1" = "val1",
        "prop2" = "val2"
    )
);
0 rows returned

--load data dataset_1;
select * from test_source_1 order by col0;
|col0|col1|col2|col3|col4|col5|col6|
|1|100|1000|1234.4321|12345678.99|str1|2020-01-01 01:00:00.123000|
|2|200|2000|2234.4321|22345678.99|str2|2020-01-02 01:00:00.123000|
|3|300|3000|3234.4321|32345678.99|str3|2020-01-03 01:00:00.123000|
|4|400|4000|4234.4321|42345678.99|str4|2020-01-04 01:00:00.123000|
|5|500|5000|5234.4321|52345678.99|str5|2020-01-05 01:00:00.123000|
|6|600|6000|6234.4321|62345678.99|str6|2020-01-06 01:00:00.123000|
|7|700|7000|7234.4321|72345678.99|str7|2020-01-07 01:00:00.123000|
|8|800|8000|8234.4321|82345678.99|str8|2020-01-08 01:00:00.123000|
|9|900|9000|9234.4321|92345678.99|str9|2020-01-09 01:00:00.123000|
|10|1000|10000|10234.4321|93345678.99|str10|2020-01-10 01:00:00.123000|
10 rows returned

-- simulate failure of Kafka - this will cause consumers to be closed and restarted;
--kafka fail testtopic test_source_1 1000;

-- reset the offsets and load the data;
--reset offsets testtopic;
--load data dataset_2;

-- only the last 5 records from dataset 2 should be present as the first 10 have the same PK as in dataset one;
select * from test_source_1 order by col0;
|col0|col1|col2|col3|col4|col5|col6|
|1|100|1000|1234.4321|12345678.99|str1|2020-01-01 01:00:00.123000|
|2|200|2000|2234.4321|22345678.99|str2|2020-01-02 01:00:00.123000|
|3|300|3000|3234.4321|32345678.99|str3|2020-01-03 01:00:00.123000|
|4|400|4000|4234.4321|42345678.99|str4|2020-01-04 01:00:00.123000|
|5|500|5000|5234.4321|52345678.99|str5|2020-01-05 01:00:00.123000|
|6|600|6000|6234.4321|62345678.99|str6|2020-01-06 01:00:00.123000|
|7|700|7000|7234.4321|72345678.99|str7|2020-01-07 01:00:00.123000|
|8|800|8000|8234.4321|82345678.99|str8|2020-01-08 01:00:00.123000|
|9|900|9000|9234.4321|92345678.99|str9|2020-01-09 01:00:00.123000|
|10|1000|10000|10234.4321|93345678.99|str10|2020-01-10 01:00:00.123000|
|11|1210|11000|11234.4321|62345678.99|str6|2020-01-06 01:00:00.123000|
|12|1310|12000|12234.4321|72345678.99|str7|2020-01-07 01:00:00.123000|
|13|1410|13000|13234.4321|82345678.99|str8|2020-01-08 01:00:00.123000|
|14|1510|14000|14234.4321|92345678.99|str9|2020-01-09 01:00:00.123000|
|15|1610|15000|15234.4321|93345678.99|str10|2020-01-10 01:00:00.123000|
15 rows returned
drop source test_source_1;
0 rows returned
--delete topic testtopic;