use test;
--create topic testtopic;
create source test_source_1(
    col0 bigint,
    col1 tinyint,
    col2 int,
    col3 double,
    col4 decimal(10, 2),
    col5 varchar,
    col6 timestamp(6),
    primary key (col0)
) with (
    brokername = "testbroker",
    topicname = "testtopic",
    headerencoding = "json",
    keyencoding = "json",
    valueencoding = "json",
    columnselectors = (
        "k.k0",
        "v.v1",
        "v.v2",
        "v.v3",
        "v.v4",
        "v.v5",
        "v.v6"
    )
    properties = (
        "prop1" = "val1",
        "prop2" = "val2"
    )
);

--load data dataset_1;
select * from test_source_1 order by col0;

-- We delete the topic, then recreate, then load it with messages with the same id (from 1-10) but different other fields
-- We then restart the cluster - the first 10 messages should be ignored due to duplicate detection as they will have the
-- same offsets;
--delete topic testtopic;
--create topic testtopic;
--restart cluster;

use test;

--load data dataset_2 no wait;
--wait for duplicates test_source_1 10;
--wait for committed test_source_1 5;
select * from test_source_1 order by col0;

drop source test_source_1;
--delete topic testtopic;